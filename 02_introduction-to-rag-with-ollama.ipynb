{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c67b89b-2af9-4931-87a0-be755c25f54d",
   "metadata": {},
   "source": [
    "# 02 Introduction to RAG with Ollama\n",
    "\n",
    "This notebook demonstrates a **Retrieval-Augmented Generation (RAG)** pipeline using the **Ollama** library for generating responses, **ChromaDB** for efficient vector-based retrieval, and **Jinja** for dynamic prompt templating. RAG combines the power of large language models with a vector database to enable dynamic and context-aware response generation based on external knowledge.\n",
    "\n",
    "### Steps Covered:\n",
    "1. **Data Preparation**: Load and preprocess text data to generate embeddings.\n",
    "2. **Embedding Creation**: Use a language model to encode textual data into vector representations.\n",
    "3. **Storage and Retrieval**: Store embeddings in ChromaDB and perform similarity-based queries.\n",
    "4. **Dynamic Prompt Generation**: Use **Jinja** templates to build structured prompts dynamically based on retrieved data.\n",
    "5. **Response Generation**: Use Ollama to generate relevant responses based on templated prompts and retrieved context.\n",
    "\n",
    "### Key Libraries:\n",
    "- **[Ollama](https://github.com/ollama/ollama-python)**: A Python library for interacting with language models for text generation and embedding.\n",
    "- **[ChromaDB](https://www.trychroma.com/)**: A vector database for storing and querying high-dimensional embeddings.\n",
    "- **[Jinja](https://jinja.palletsprojects.com/)**: A templating engine for building flexible and reusable text prompts.\n",
    "\n",
    "This step-by-step guide provides a hands-on approach to building and understanding the RAG pipeline, leveraging modularity and clarity for easy adaptation into your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18560a92-a1a8-47e2-96af-a06501c490cc",
   "metadata": {},
   "source": [
    "## 02.01 Installing Required Libraries\n",
    "\n",
    "This cell installs the necessary Python libraries for implementing the RAG pipeline.\n",
    "\n",
    "1. **`ollama`**:\n",
    "   - A library for interacting with large language models.\n",
    "   - Enables functionalities like generating embeddings and responses.\n",
    "   - Official documentation: [Ollama GitHub](https://github.com/ollama/ollama-python).\n",
    "\n",
    "2. **`chromadb`**:\n",
    "   - A high-performance vector database for storing and retrieving embeddings.\n",
    "   - Used for similarity searches in the RAG pipeline.\n",
    "   - Official website: [ChromaDB](https://www.trychroma.com/).\n",
    "\n",
    "3. **`Jinja2`**:\n",
    "   - A templating engine for dynamically generating text prompts based on retrieved data.\n",
    "   - Simplifies creating structured, reusable prompts.\n",
    "   - Official documentation: [Jinja2 Documentation](https://jinja.palletsprojects.com/).\n",
    "\n",
    "### Why `%pip`?\n",
    "- The `%pip install` command ensures that the libraries are installed in the current Jupyter notebook environment, preventing conflicts with other Python environments.\n",
    "\n",
    "**Note**: Run this cell only once to install the libraries. Re-running it after the libraries are installed will skip the installation process if the packages are already up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e705ed-4c75-403c-87fd-c90119b6a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.01 Installing Required Libraries\n",
    "\n",
    "# Installing the Ollama library for interaction with large language models.\n",
    "%pip install ollama\n",
    "\n",
    "# Installing ChromaDB, a vector database for storing and retrieving high-dimensional embeddings.\n",
    "%pip install chromadb\n",
    "\n",
    "# Installing Jinja2, a templating engine for dynamically generating structured prompts.\n",
    "%pip install Jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfe83a-d747-45bb-a96e-db0070e5262e",
   "metadata": {},
   "source": [
    "## 02.02 Configuring the Embedding Model\n",
    "\n",
    "This section sets up the **Ollama Embedding Function** for generating vector representations, which will be used by ChromaDB for similarity-based retrieval.\n",
    "\n",
    "### Steps and Requirements:\n",
    "1. **Importing Embedding Functions**:\n",
    "   - The `chromadb.utils.embedding_functions` module provides utilities for defining embedding functions compatible with ChromaDB.\n",
    "\n",
    "2. **Embedding Model Configuration**:\n",
    "   - The `OllamaEmbeddingFunction` is configured with:\n",
    "     - `url`: The API endpoint for Ollama's embedding service.\n",
    "     - `model_name`: Specifies the embedding model, `nomic-embed-text`, for vector generation.\n",
    "   - Make sure the `nomic-embed-text` embedding model is installed and running.\n",
    "\n",
    "### Additional Resources:\n",
    "- **Supported Embedding Models**:\n",
    "  - Refer to the [Ollama Supported Embedding Models](https://ollama.com/search?c=embedding) for a complete list of embedding models.\n",
    "  - Install the required embedding model before proceeding.\n",
    "- **Embedding Overview**:\n",
    "  - Learn more about embeddings and their applications in [IBM's Documentation](https://www.ibm.com/think/topics/embedding).\n",
    "\n",
    "### How to Verify Installed Models:\n",
    "To check all the embedding models installed and available in Ollama, run:\n",
    "```python\n",
    "import ollama\n",
    "available_models = ollama.list()\n",
    "for model in available_models.models:\n",
    "    print(model.model)\n",
    "```\n",
    "\n",
    "Ensure that the `nomic-embed-text:latest` model is listed in the output, else either install this model or use an existing model.\n",
    "\n",
    "### Why Embeddings?\n",
    "\n",
    "Embedding models convert textual data into high-dimensional vectors, enabling efficient similarity searches in a vector database like ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fa9ad-aab7-4572-a883-a574e7b72f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.02 Configuring the Embedding Model\n",
    "\n",
    "# Importing the embedding functions utility from ChromaDB.\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# Setting up an Ollama embedding function for ChromaDB.\n",
    "# Ensure that the embedding model 'nomic-embed-text' is installed and available.\n",
    "# This will utilize Ollama's embedding API for generating vector representations.\n",
    "ollama_embedding = embedding_functions.OllamaEmbeddingFunction(\n",
    "    url=\"http://localhost:11434/api/embeddings\",  # API endpoint for Ollama's embedding service.\n",
    "    model_name=\"nomic-embed-text:latest\"  # The embedding model to be used for generating vectors.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dc459-eb11-459a-bf2d-f7b1ad124496",
   "metadata": {},
   "source": [
    "## 02.03 Generating Sample Embeddings\n",
    "\n",
    "This section demonstrates how to generate embeddings for a sample text using the configured Ollama embedding function.\n",
    "\n",
    "### Steps:\n",
    "1. **Input Text**:\n",
    "   - A sample text is passed as a list to the `ollama_embedding` function. Each string in the list will be converted into a high-dimensional vector.\n",
    "\n",
    "2. **Generating Embeddings**:\n",
    "   - The `OllamaEmbeddingFunction` processes the input text and returns its vector representation.\n",
    "\n",
    "3. **Output**:\n",
    "   - The output, `sample_embeddings`, is a list of vectors corresponding to the input text. These embeddings are suitable for tasks like similarity searches, clustering, or input to machine learning models.\n",
    "\n",
    "### Example Use Case:\n",
    "Given the input text:\n",
    "```python\n",
    "\"This is a sample text to try Ollama embedding at the workshop\"\n",
    "```\n",
    "The output will be a vector, such as:\n",
    "```\n",
    "[[0.1, 0.2, 0.3, ...]]  # Example representation of a high-dimensional vector.\n",
    "```\n",
    "### Why Embeddings Are Useful:\n",
    "\n",
    "- **Dimensionality Reduction**: Transform text into numerical representations for computations.\n",
    "- **Similarity Search**: Enable searching for semantically similar texts in a vector database like ChromaDB.\n",
    "- **Downstream Applications**: Useful in clustering, classification, or as input to models.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Ensure that the embedding model (`nomic-embed-text:latest`) is properly installed and running.\n",
    "- Embeddings are high-dimensional numerical data, so the output may not be human-readable but is critical for computational tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed346646-53b8-488d-8995-e346f793e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.03 Generating Sample Embeddings\n",
    "\n",
    "# Generating embeddings for a sample text using the configured Ollama embedding function.\n",
    "# The input is a list of strings, and the function returns corresponding vector representations.\n",
    "sample_embeddings = ollama_embedding(\n",
    "    [\"This is a sample text to try Ollama embedding at the workshop\"]\n",
    ")\n",
    "\n",
    "# Printing the generated embeddings to verify the output.\n",
    "# Note: The output is a high-dimensional vector representation of the input text.\n",
    "print(sample_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fc81d-1dc4-49ef-911a-ebdbf3663415",
   "metadata": {},
   "source": [
    "## 02.04 Using a Persistent Vector Database with ChromaDB\n",
    "\n",
    "This section explains using a **Persistent Database** with ChromaDB and its configuration for this demonstration.\n",
    "\n",
    "### Why Use a Persistent Database?\n",
    "\n",
    "1. **Efficiency**:\n",
    "   - When working with large datasets, generating embeddings and indexing them can be computationally expensive and time-consuming.\n",
    "   - A persistent database ensures that the embeddings and indexes are saved to disk, avoiding the need to re-index data every time the notebook is run.\n",
    "\n",
    "2. **Reusability**:\n",
    "   - Once the database is created, it can be reused across multiple sessions or notebooks.\n",
    "   - You can load the database and immediately perform similarity queries without regenerating embeddings.\n",
    "\n",
    "3. **Performance**:\n",
    "   - ChromaDB's persistent storage is optimized for rapid retrieval of embeddings, enabling faster searches in subsequent runs.\n",
    "\n",
    "### How ChromaDB Saves Data\n",
    "ChromaDB saves embeddings, indexes, and metadata on disk at the specified path (`./vector-db/made-with-cc` in this case). \n",
    "- The data is stored in a structured format that supports efficient querying and retrieval.\n",
    "- By using persistent storage, ChromaDB ensures data consistency and eliminates the need for in-memory reinitialization.\n",
    "\n",
    "### Demonstration Example\n",
    "\n",
    "For this demonstration, we use the book **\"Made with Creative Commons\"**:\n",
    "- **Source**: [Made with Creative Commons](https://creativecommons.org/share-your-work/made-with-cc/)\n",
    "- **License**: The book is licensed under Creative Commons Attribution-Sharealike 4.0 International license, allowing free use and sharing with attribution.\n",
    "\n",
    "#### Prepared Data:\n",
    "- The text of the book has been pre-processed:\n",
    "  - Paragraphs were extracted line by line and saved into a text file: `made-with-cc.txt`.\n",
    "  - This file is stored in the current working directory.\n",
    "  \n",
    "#### Notes:\n",
    "- For your own projects, you may need to preprocess your data and create text files containing paragraphs or other units of text for embedding and indexing.\n",
    "- This demonstration does not cover text extraction or preparation techniques.\n",
    "\n",
    "### Persistent Database Initialization\n",
    "In this cell:\n",
    "- **`chromadb.PersistentClient()`**:\n",
    "  - Configures a client with persistent storage.\n",
    "  - Saves the database at `./vector-db/made-with-cc`.\n",
    "  - When this notebook is rerun, the database will be reloaded from disk without requiring re-indexing.\n",
    "\n",
    "### Key Advantages:\n",
    "- Saves time by avoiding redundant computations.\n",
    "- Simplifies workflows by separating data preparation from query execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a5649-c72f-415a-ba31-029a280e1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.04 Using a Persistent Vector Database with ChromaDB\n",
    "\n",
    "# Importing ChromaDB, a vector database for efficient embedding storage and retrieval.\n",
    "import chromadb\n",
    "\n",
    "# Initializing a Persistent ChromaDB client.\n",
    "# The database will be stored persistently on disk at the specified path.\n",
    "cc_client = chromadb.PersistentClient(path=\"./vector-db/made-with-cc\")  # Path to the persistent database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ab724-ad7c-4afd-bbab-1274176efb7c",
   "metadata": {},
   "source": [
    "## 02.05 Helper Functions for Data Processing\n",
    "\n",
    "This section defines a set of helper functions to streamline the process of loading and embedding data into the vector database.\n",
    "\n",
    "### Functions Overview:\n",
    "\n",
    "1. **`generate_line_number_id(index)`**:\n",
    "   - Creates a unique ID for each line in the input file based on its line number (1-based index).\n",
    "   - Helps in tracking and referencing individual text entries.\n",
    "\n",
    "2. **`get_embedding_for_document(document)`**:\n",
    "   - Generates a vector embedding for a given text document using the configured `ollama_embedding` function.\n",
    "   - Includes error handling to catch and report issues during the embedding process.\n",
    "\n",
    "3. **`load_documents_with_line_ids_and_embeddings(file_path, collection)`**:\n",
    "   - Loads documents from a text file, assigns a line-based ID to each, generates embeddings, and adds them to the specified ChromaDB collection.\n",
    "   - Steps:\n",
    "     - Reads the text file line by line, skipping empty lines.\n",
    "     - Generates unique IDs for each document.\n",
    "     - Uses `get_embedding_for_document` to compute embeddings sequentially.\n",
    "     - Adds the documents, IDs, and embeddings to the ChromaDB collection using `collection.upsert()`.\n",
    "\n",
    "### Key Points:\n",
    "- **Error Handling**: Gracefully handles file not found errors and embedding failures.\n",
    "- **Data Processing**: Each document is processed line by line, ensuring clean and consistent indexing.\n",
    "- **Embedding Generation**: Embeddings are generated using the pre-configured `ollama_embedding` function.\n",
    "\n",
    "### Example Usage:\n",
    "```python\n",
    "# Assuming 'my_collection' is a ChromaDB collection object.\n",
    "load_documents_with_line_ids_and_embeddings(\"made-with-cc.txt\", my_collection)\n",
    "```\n",
    "This approach simplifies loading and embedding data into the database, making it easy to adapt for larger or custom datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6dc567-c441-4117-905c-42b7fa70882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.05 Helper Functions for Data Processing\n",
    "\n",
    "def generate_line_number_id(index):\n",
    "    \"\"\"\n",
    "    Generates an ID based on the line number.\n",
    "\n",
    "    Args:\n",
    "        index (int): The zero-based index of the line in the file.\n",
    "\n",
    "    Returns:\n",
    "        str: The line number ID as a string (1-based index).\n",
    "    \"\"\"\n",
    "    return str(index + 1)\n",
    "\n",
    "def get_embedding_for_document(document):\n",
    "    \"\"\"\n",
    "    Generates an embedding for a given document using the `ollama_embedding` function.\n",
    "\n",
    "    Args:\n",
    "        document (str): The document text.\n",
    "\n",
    "    Returns:\n",
    "        list: The embedding for the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ollama_embedding([document])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for document: {document[:30]}... Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_documents_with_line_ids_and_embeddings(file_path, collection):\n",
    "    \"\"\"\n",
    "    Loads documents from a text file, assigns a line number as the ID to each document,\n",
    "    generates embeddings for each document, and adds them to a collection.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file containing documents (one per line).\n",
    "        collection (object): The collection object to which the documents, IDs, and embeddings will be added.\n",
    "\n",
    "    Functionality:\n",
    "        - Reads a text file line by line.\n",
    "        - Strips whitespace from each line and skips empty lines.\n",
    "        - Generates a line number ID for each document.\n",
    "        - Generates embeddings for each document sequentially.\n",
    "        - Adds the documents, IDs, and embeddings to the collection using `collection.upsert()`.\n",
    "\n",
    "    Example Usage:\n",
    "        collection = some_vector_database.collection(\"made-with-cc\")\n",
    "        load_documents_with_line_ids_and_embeddings(\"made-with-cc.txt\", collection)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open and read the text file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Process each line to strip whitespace and remove empty entries\n",
    "        documents = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        # Generate line number IDs for each document\n",
    "        ids = [generate_line_number_id(i) for i in range(len(documents))]\n",
    "\n",
    "        # Generate embeddings sequentially\n",
    "        embeddings = []\n",
    "        for doc in documents:\n",
    "            embedding = get_embedding_for_document(doc)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                embeddings.append([])  # Append an empty list for documents that fail\n",
    "\n",
    "        # Add the documents, IDs, and embeddings to the collection\n",
    "        collection.upsert(documents=documents, ids=ids, embeddings=embeddings)\n",
    "\n",
    "        print(f\"Successfully added {len(documents)} documents with embeddings to the collection.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332e1f3-e972-44e0-8209-f8bffb1a0729",
   "metadata": {},
   "source": [
    "## 02.06 Initializing and Loading the ChromaDB Collection\n",
    "\n",
    "This section demonstrates how to initialize or retrieve a ChromaDB collection and ensure it is populated with data for querying.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Create or Retrieve a Collection**:\n",
    "   - The `get_or_create_collection()` method ensures that a collection named `\"made-with-cc\"` exists in the database.\n",
    "   - If the collection does not already exist, it will be created.\n",
    "\n",
    "2. **Check if the Collection is Empty**:\n",
    "   - The `count()` method checks the number of documents currently stored in the collection.\n",
    "   - If the collection is empty (`count() == 0`):\n",
    "     - A message is displayed, and the helper function `load_documents_with_line_ids_and_embeddings` is called to load the data.\n",
    "     - This step processes the `made-with-cc.txt` file and populates the collection with documents and embeddings.\n",
    "   - If the collection is not empty:\n",
    "     - A message is displayed indicating the collection is ready for querying, along with the current document count.\n",
    "\n",
    "### Example Usage:\n",
    "- When the notebook is run for the first time, the collection will be populated with embeddings from `made-with-cc.txt`.\n",
    "- Subsequent runs will skip reloading if the collection is already populated, saving time and resources.\n",
    "\n",
    "### Benefits:\n",
    "- **Persistence**:\n",
    "  - Leveraging ChromaDB's persistent storage, this approach avoids redundant re-indexing when the notebook is rerun.\n",
    "- **Efficiency**:\n",
    "  - Ensures that embeddings are only generated once, making the pipeline faster for future queries.\n",
    "  \n",
    "### Output Messages:\n",
    "- For an empty collection:\n",
    "  ```\n",
    "  Collection ‘made-with-cc’ is empty. Loading documents… Please wait…\n",
    "  ```\n",
    "- For a preloaded collection:\n",
    "  ```\n",
    "  Loaded collection ‘made-with-cc’. Collection contains X documents.\n",
    "  This collection is ready to be queried.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277eb795-443c-4d9a-96c6-6e2fdc0c79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.06 Initializing and Loading the ChromaDB Collection\n",
    "\n",
    "# Create or retrieve a collection named \"made-with-cc\" in the persistent ChromaDB database.\n",
    "cc_collection = cc_client.get_or_create_collection(name=\"made-with-cc\")\n",
    "\n",
    "# Check if the collection is empty.\n",
    "if cc_collection.count() == 0:\n",
    "    # If the collection is empty, print a message and load the documents.\n",
    "    print(f\"Collection 'made-with-cc' is empty. Loading documents... Please wait...\")\n",
    "    load_documents_with_line_ids_and_embeddings(\"made-with-cc.txt\", cc_collection)\n",
    "else:\n",
    "    # If the collection is not empty, print the current document count.\n",
    "    print(f\"Loaded collection 'made-with-cc'. Collection contains {cc_collection.count()} documents. \\nThis collection is ready to be queried.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e7169-3a21-4aac-a20d-988b6729a010",
   "metadata": {},
   "source": [
    "## 02.07 Querying the Database for Matching Concepts\n",
    "\n",
    "This section demonstrates how to query the ChromaDB collection to retrieve documents that match a specific concept based on vector embeddings.\n",
    "\n",
    "### Example Prompt\n",
    "For this demonstration, the query prompt is:\n",
    "\n",
    "```\n",
    "How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\n",
    "```\n",
    "\n",
    "This prompt reflects a conceptual query designed to retrieve semantically similar paragraphs from the database.\n",
    "\n",
    "### Understanding Similarity and Distance\n",
    "- **Concept of Distance**:\n",
    "  - Distance is a numerical measure of how closely the query embedding matches document embeddings in the database.\n",
    "  - A **smaller distance** indicates a higher similarity between the query and the document's concept.\n",
    "  - A **larger distance** suggests that the document is less relevant to the given query.\n",
    "\n",
    "- **How It Works**:\n",
    "  - Each paragraph in the database is represented as a high-dimensional vector (embedding).\n",
    "  - The query is also converted into a vector using the same embedding function.\n",
    "  - The database computes the distance between the query vector and each document vector, returning the documents with the smallest distances.\n",
    "\n",
    "- **Practical Meaning**:\n",
    "  - The closer the distance, the more aligned the document's idea or concept is with the given query.\n",
    "  - This allows for semantic (concept-based) matching rather than simple keyword matching.\n",
    "\n",
    "### Query Process\n",
    "1. **Query Embeddings**:\n",
    "   - The prompt is converted into vector embeddings using the preconfigured `ollama_embedding` function.\n",
    "   - These embeddings serve as the basis for similarity matching.\n",
    "\n",
    "2. **ChromaDB Query**:\n",
    "   - The `cc_collection.query()` method is used to search the database for the top `n_results` (10 in this case) matching documents based on similarity.\n",
    "\n",
    "3. **Output Format**:\n",
    "   - Results include:\n",
    "     - Document IDs: Unique identifiers for the matched paragraphs.\n",
    "     - Distances: Numerical values representing the similarity between the query and the results.\n",
    "     - Paragraph Text: The actual content of the matched documents.\n",
    "\n",
    "### Advanced Output Rendering\n",
    "- **HTML Table**:\n",
    "  - Results are presented in a styled HTML table using Jupyter's `IPython.display.HTML` for better readability.\n",
    "  - The table includes columns for:\n",
    "    - Document ID (Paragraph Number).\n",
    "    - Distance (Similarity Score).\n",
    "    - Paragraph Text (Matched Content).\n",
    "\n",
    "- **Plain Text Option**:\n",
    "  - If you prefer, uncomment the following lines to print the results in plain text format:\n",
    "  ```python\n",
    "  for id_num, document, distance in zip(ids, documents, distances):\n",
    "      print(f\"\\n[{id_num}] [{distance}] {document}\")\n",
    "  ```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- **Efficiency**: ChromaDB retrieves matches quickly, leveraging its persistent indexing.\n",
    "- **Contextual Matching**: Embeddings allow semantic similarity searches, providing results even for abstract or indirect matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea685bc-9c09-4f4d-b1cb-f2fff23968b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.07 Querying the Database for Matching Concepts\n",
    "\n",
    "# Query the ChromaDB collection for documents matching the given concept.\n",
    "results = cc_collection.query(\n",
    "    query_embeddings=ollama_embedding([\n",
    "        \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\"\n",
    "    ]),\n",
    "    n_results=10  # Number of top matching results to return.\n",
    ")\n",
    "\n",
    "# Extract results data\n",
    "ids = results['ids'][0]  # Extract the IDs of the matching documents.\n",
    "documents = results['documents'][0]  # Extract the document content of the matches.\n",
    "distances = results['distances'][0]  # Extract the distances between the query and matches.\n",
    "\n",
    "# We will be using Jupyter notebook's Advanced options to render this result as HTML \n",
    "# but if you prefer to render them directly as it is in plain text you can uncomment the below two lines.\n",
    "\n",
    "#for id_num, document, distance in zip(ids, documents, distances):\n",
    "#    print(f\"\\n[{id_num}] [{distance}] {document}\")\n",
    "\n",
    "# Jupyter advanced options: Render results as an HTML table\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Prepare data for the HTML table\n",
    "data = [[id_num, round(distance, 3), doc] for id_num, distance, doc in zip(ids, distances, documents)]\n",
    "\n",
    "# Define an HTML table with styling\n",
    "html_table = '''\n",
    "<table style=\"width:100%; table-layout: auto; border-collapse: collapse;\">\n",
    "    <colgroup>\n",
    "        <col style=\"white-space: nowrap;\"> <!-- Paragraph Number column -->\n",
    "        <col style=\"white-space: nowrap;\"> <!-- Distance column -->\n",
    "        <col style=\"width: auto;\"> <!-- Paragraph Text column -->\n",
    "    </colgroup>\n",
    "    <tr style=\"background-color: #f8f9fa;\">\n",
    "        <th style=\"border: 1px solid #dee2e6; padding: 8px;\">No</th>\n",
    "        <th style=\"border: 1px solid #dee2e6; padding: 8px;\">Distance</th>\n",
    "        <th style=\"border: 1px solid #dee2e6; padding: 8px; text-align: left;\">Paragraph Text</th>\n",
    "    </tr>\n",
    "    <tr>{}</tr>\n",
    "</table>\n",
    "'''.format(\n",
    "    '</tr><tr>'.join(\n",
    "        '<td style=\"border: 1px solid #dee2e6; padding: 8px; text-align: center;\">{}</td><td style=\"border: 1px solid #dee2e6; padding: 8px; text-align: center;\">{}</td><td style=\"border: 1px solid #dee2e6; padding: 12px; text-align: justify;\">{}</td>'.format(*row) for row in data\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the table\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbe110-dc46-4ae2-b3dc-307e9b82119c",
   "metadata": {},
   "source": [
    "## 02.08 Streaming and Displaying Markdown Responses from Ollama\n",
    "\n",
    "This section provides a helper function to stream responses from an Ollama model and display them dynamically in Markdown format within a Jupyter Notebook. The alternative option is directly using the `chat` function for static outputs.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "1. **`stream_markdown()`**:\n",
    "   - Streams a response from the specified model and input prompt.\n",
    "   - Dynamically updates the displayed content in Markdown as the response is received.\n",
    "\n",
    "2. **Core Features**:\n",
    "   - **Streaming**: The response is processed in chunks using Ollama's `chat` API with the `stream=True` option.\n",
    "   - **Markdown Display**: Chunks are rendered as Markdown in real time using Jupyter's `display()` and `Markdown()` functions.\n",
    "   - **Update Control**:\n",
    "     - Updates the display every 300 milliseconds (configurable via `UPDATE_INTERVAL`) or if the buffer exceeds 500 characters.\n",
    "     - Ensures smooth and responsive streaming in the notebook.\n",
    "\n",
    "### Usage Example\n",
    "```python\n",
    "response = stream_markdown(\n",
    "    'llama3.2:3b',  # Model name\n",
    "    'How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?'  # Prompt\n",
    ")\n",
    "```\n",
    "\n",
    "The response will be streamed and displayed dynamically in Markdown, making it more readable and interactive.\n",
    "\n",
    "### Alternative Option\n",
    "\n",
    "- For static responses without streaming:\n",
    "Use the `chat()` function directly and display the response after processing:\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': 'How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?'}]\n",
    ")\n",
    "\n",
    "display(Markdown(response['message']['content']))\n",
    "```\n",
    "\n",
    "- Print the stream as it is:\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "stream = chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': 'How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n",
    "```\n",
    "\n",
    "### Benefits of Streaming\n",
    "\n",
    "- Real-time feedback during long responses.\n",
    "- Improved interactivity for exploratory tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90973999-8c4e-4f0e-885e-93ac9bfa4ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 02.08 Streaming and Displaying Markdown Responses from Ollama\n",
    "\n",
    "from IPython.display import display, Markdown  # For rendering Markdown in Jupyter Notebook\n",
    "from ollama import chat  # For interacting with the Ollama chat API\n",
    "import time  # For controlling update intervals in the streaming output\n",
    "\n",
    "def stream_markdown(model, prompt):\n",
    "    \"\"\"\n",
    "    Streams a response from an Ollama chat model and displays it as Markdown in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model to query.\n",
    "        prompt (str): The input prompt for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The full response streamed from the model.\n",
    "    \"\"\"\n",
    "    # Start the streaming chat with the specified model and user-provided prompt\n",
    "    stream = chat(\n",
    "        model=model,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize variables for tracking the response and buffer updates\n",
    "    full_response = \"\"  # Stores the complete response\n",
    "    buffer = \"\"  # Temporarily stores incoming chunks\n",
    "    last_update = time.time()  # Tracks the last time the display was updated\n",
    "    UPDATE_INTERVAL = 0.3  # Minimum time interval (in seconds) for updates\n",
    "    \n",
    "    try:\n",
    "        # Process the streamed response chunk by chunk\n",
    "        for chunk in stream:\n",
    "            chunk_content = chunk['message']['content']  # Extract the text content from the chunk\n",
    "            buffer += chunk_content  # Append new content to the buffer\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Update the Markdown display if enough time has passed or buffer is too large\n",
    "            if (current_time - last_update > UPDATE_INTERVAL) or len(buffer) > 500:\n",
    "                full_response += buffer  # Add buffered content to the full response\n",
    "                display(Markdown(full_response), clear=True)  # Render the Markdown\n",
    "                buffer = \"\"  # Clear the buffer\n",
    "                last_update = current_time\n",
    "        \n",
    "        # Final update to display any remaining content in the buffer\n",
    "        if buffer:\n",
    "            full_response += buffer\n",
    "            display(Markdown(full_response), clear=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")  # Print any errors encountered during streaming\n",
    "        \n",
    "    return full_response  # Return the complete response as a string\n",
    "\n",
    "# Usage Example\n",
    "response = stream_markdown(\n",
    "    'llama3.2:3b',  # Model name\n",
    "    'Explain what the llama3.2 model is and its main purpose in simple terms as a table.'  # Prompt for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b75e83-3f96-4ce4-9a71-4de9a078ee5b",
   "metadata": {},
   "source": [
    "## 02.09 Querying the Default Model Knowledge\n",
    "\n",
    "In this section, we query the model using a simple prompt without providing any additional context or external data. This demonstrates how the model generates responses based solely on its default training and internal knowledge.\n",
    "\n",
    "### Purpose\n",
    "This section serves as a **baseline** for comparison:\n",
    "- By observing the output generated without additional context, we can later compare it to outputs produced with the **RAG pipeline**.\n",
    "- This highlights how including external data (retrieved from ChromaDB in RAG) enriches the model's responses.\n",
    "\n",
    "### Example Prompt\n",
    "For this demonstration, the prompt is:\n",
    "\n",
    "```\n",
    "“How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?”\n",
    "```\n",
    "\n",
    "This query is designed to explore how Creative Commons licenses impact the digital commons, balancing open sharing with market-based approaches.\n",
    "\n",
    "### Usage\n",
    "The `stream_markdown()` function streams the model's response dynamically and displays it in Markdown format. The response reflects the model's understanding of the topic based on its pre-trained knowledge.\n",
    "\n",
    "### Key Insights\n",
    "- **Default Knowledge**: The model's response is limited to what it has learned during training. It does not incorporate external or real-time context.\n",
    "- **Reference Output**: The generated output serves as a reference point to evaluate the impact of augmenting the model with external data using RAG.\n",
    "\n",
    "### Next Step\n",
    "In subsequent sections, we will use the RAG pipeline to provide additional context to the model, compare the results, and demonstrate the value of integrating external knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ebea7-e801-4f05-87fd-001b4cf636f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#02.09 Querying the Default Model Knowledge\n",
    "\n",
    "query = \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\"\n",
    "response = stream_markdown(\n",
    "    'llama3.2:3b',  # Model name\n",
    "    query  # Prompt for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebfe75-7c49-446e-aea0-1ac7aa633fd6",
   "metadata": {},
   "source": [
    "## 02.10 Retrieval-Augmented Generation (RAG) Example\n",
    "\n",
    "In this section, we use the **RAG pipeline** to enhance the model's output by providing it with additional context retrieved from a vector database. This demonstrates how augmenting the model with external knowledge improves the relevance and specificity of its responses.\n",
    "\n",
    "### Steps\n",
    "1. **Define the Query**:\n",
    "   - The query is a conceptual question:\n",
    "     ```\n",
    "     \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\"\n",
    "     ```\n",
    "\n",
    "2. **Retrieve Related Paragraphs**:\n",
    "   - Using ChromaDB, we query the vector database to retrieve the top 10 paragraphs most relevant to the query.\n",
    "   - These paragraphs are selected based on their semantic similarity to the query embeddings.\n",
    "\n",
    "3. **Construct the RAG Prompt**:\n",
    "   - The retrieved paragraphs are combined with the query to create a context-rich prompt:\n",
    "     ```\n",
    "     \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms? - Answer using these references: [retrieved paragraphs]\"\n",
    "     ```\n",
    "\n",
    "4. **Stream the Response**:\n",
    "   - The `stream_markdown()` function sends the context-enhanced prompt to the model (`llama3.2:3b`) and streams the generated response.\n",
    "\n",
    "### Benefits of RAG\n",
    "- **Context-Aware Responses**:\n",
    "  - By incorporating retrieved references, the model can provide more specific and informed answers.\n",
    "- **Relevance**:\n",
    "  - The retrieved paragraphs ground the model's response in external data, ensuring alignment with the provided context.\n",
    "\n",
    "### Comparison with Default Knowledge\n",
    "- Unlike the earlier section where the model relied solely on its pre-trained knowledge, this approach augments its capabilities with up-to-date and domain-specific data.\n",
    "\n",
    "### Example Output\n",
    "For the query:\n",
    "\n",
    "```\n",
    "“How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?”\n",
    "```\n",
    "\n",
    "The RAG-enhanced response will be more contextually accurate, referencing the retrieved paragraphs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d02b4-4722-41d3-ab88-f841c535003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.10 Retrieval-Augmented Generation (RAG) Example\n",
    "\n",
    "query = \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\"\n",
    "\n",
    "related_paragraphs = cc_collection.query(\n",
    "    query_embeddings=ollama_embedding([\n",
    "        query\n",
    "    ]),\n",
    "    n_results=10  # Number of top matching results to return.\n",
    ")['documents'][0]\n",
    "\n",
    "basic_rag_prompt = f\"{query} - Answer using these references: {' '.join(related_paragraphs)}\"\n",
    "\n",
    "response = stream_markdown(\n",
    "    'llama3.2:3b',  # Model name\n",
    "    basic_rag_prompt  # RAG Prompt for the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb289da-e88f-420d-931e-8a140f2e6141",
   "metadata": {},
   "source": [
    "## 02.11 Enhanced RAG Prompt Generation Using Jinja2\n",
    "\n",
    "This section demonstrates how to create a detailed and structured prompt for **Retrieval-Augmented Generation (RAG)** using Jinja2 templates. The enhanced prompt incorporates retrieved contextual information with clear instructions and paragraph citations.\n",
    "\n",
    "### Key Features of the RAG Template\n",
    "- **Context Integration**:\n",
    "  - Includes the retrieved paragraphs with paragraph IDs (e.g., `[p1]`).\n",
    "  - Ensures the model has clear and structured context to generate responses.\n",
    "- **Instructions**:\n",
    "  - Guides the model to:\n",
    "    - Prioritize the provided context.\n",
    "    - Use logical flow and coherence.\n",
    "    - Cite paragraphs explicitly using `[p{number}]` notation.\n",
    "    - Acknowledge gaps if relevant information is missing.\n",
    "    - Synthesize multiple paragraphs for comprehensive answers.\n",
    "\n",
    "### `create_rag_prompt` Function\n",
    "- **Purpose**:\n",
    "  - Combines a user query and retrieved context into a formatted RAG prompt.\n",
    "- **Validation**:\n",
    "  - Ensures the `context_docs` dictionary contains both `documents` and `ids`.\n",
    "  - Checks that both lists are non-empty and aligned in length.\n",
    "- **Formatting**:\n",
    "  - Formats each paragraph with its corresponding ID.\n",
    "  - Joins all paragraphs into a cohesive block of context.\n",
    "  - Renders the final prompt using the Jinja2 template.\n",
    "\n",
    "### Example Workflow\n",
    "1. **Retrieve Context**:\n",
    "   - Use a vector database like ChromaDB to retrieve paragraphs related to the query.\n",
    "2. **Generate Prompt**:\n",
    "   - Use the `create_rag_prompt` function to combine the query and retrieved paragraphs.\n",
    "3. **Example Query**:\n",
    "   ```\n",
    "   “How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?”\n",
    "   ```\n",
    "4. **Generated Prompt**:\n",
    "The output includes the question, formatted context, and clear instructions.\n",
    "\n",
    "### Benefits\n",
    "- **Improved Clarity**:\n",
    "  Structured prompts guide the model to produce focused and accurate responses.\n",
    "- **Enhanced Citation**:\n",
    "  Paragraph IDs ensure traceability of information sources.\n",
    "- **Reusability**:\n",
    "  Jinja2 templates allow for easy customization and scalability for other use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab68986-0ef7-412b-a1e0-caea7130f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 02.11 Enhanced RAG Prompt Generation Using Jinja2\n",
    "\n",
    "from jinja2 import Template  # Importing Jinja2 for creating and rendering templates.\n",
    "\n",
    "# Enhanced RAG template with instructions for answering using context.\n",
    "rag_template = Template(\"\"\"\n",
    "You are a knowledgeable assistant tasked with answering questions based on provided context.\n",
    "Focus on providing accurate, relevant information from the given sources while maintaining logical flow.\n",
    "\n",
    "Context:\n",
    "{{ context }}\n",
    "\n",
    "Question: {{ question }}\n",
    "\n",
    "Instructions:\n",
    "- Answer based primarily on the provided context\n",
    "- Maintain coherent logical flow\n",
    "- Cite specific paragraphs using [p{number}] notation when referencing information\n",
    "- If information is missing from context, acknowledge this\n",
    "- Synthesize information across multiple paragraphs when relevant\n",
    "- Use paragraph numbers to build a cohesive narrative while maintaining accurate citations\n",
    "\"\"\")\n",
    "\n",
    "def create_rag_prompt(query, context_docs):\n",
    "    \"\"\"\n",
    "    Creates an enhanced RAG prompt combining the query and relevant context with paragraph IDs.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question\n",
    "        context_docs (dict): Retrieved documents from vector search containing:\n",
    "            - documents: list of document content\n",
    "            - ids: list of paragraph IDs\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated prompt with context and instructions.\n",
    "    \"\"\"\n",
    "    # Validate input structure\n",
    "    if not all(key in context_docs for key in [\"documents\", \"ids\"]):\n",
    "        raise ValueError(\"context_docs must contain both 'documents' and 'ids' keys\")\n",
    "    \n",
    "    if not context_docs[\"documents\"] or not context_docs[\"ids\"]:\n",
    "        raise ValueError(\"Both documents and ids lists must not be empty\")\n",
    "        \n",
    "    if len(context_docs[\"documents\"][0]) != len(context_docs[\"ids\"][0]):\n",
    "        raise ValueError(\"Number of documents must match number of ids\")\n",
    "\n",
    "    # Combine documents with their paragraph IDs\n",
    "    context_entries = []\n",
    "    for doc, para_id in zip(context_docs[\"documents\"][0], context_docs[\"ids\"][0]):\n",
    "        # Format each context entry with its paragraph ID\n",
    "        context_entries.append(f\"[p{para_id}] {doc.strip()}\")\n",
    "    \n",
    "    # Join all retrieved documents with clear separation\n",
    "    context = \"\\n\\n\".join(context_entries)\n",
    "    \n",
    "    # Generate prompt using template\n",
    "    prompt = rag_template.render(\n",
    "        question=query,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example usage\n",
    "query = \"How do Creative Commons licenses contribute to fostering the digital commons and balancing it with market norms?\"\n",
    "\n",
    "# Retrieve relevant documents using vector search.\n",
    "related_paragraphs = cc_collection.query(\n",
    "    query_embeddings=ollama_embedding([query]),\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "# Create an improved RAG prompt using the retrieved paragraphs.\n",
    "improved_rag_prompt = create_rag_prompt(query, related_paragraphs)\n",
    "\n",
    "# Display the prompt for inspection.\n",
    "print(improved_rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938bda3-de33-4dda-8d13-62eb55132e3c",
   "metadata": {},
   "source": [
    "## 02.12 Generating a Response with the Improved RAG Prompt\n",
    "\n",
    "This section demonstrates how to query the model using the **Improved RAG Prompt**. The enhanced prompt incorporates context retrieved from a vector database, along with structured instructions to guide the model's response generation.\n",
    "\n",
    "### Workflow\n",
    "1. **Improved Prompt**:\n",
    "   - The `improved_rag_prompt` is generated using the `create_rag_prompt` function.\n",
    "   - It combines:\n",
    "     - The user query.\n",
    "     - Context paragraphs retrieved using vector search.\n",
    "     - Explicit instructions on how the model should use the provided context.\n",
    "\n",
    "2. **Model Query**:\n",
    "   - The `stream_markdown()` function sends the enhanced RAG prompt to the model (`llama3.2:3b`).\n",
    "   - The response is streamed and displayed dynamically in Markdown format.\n",
    "\n",
    "### Purpose\n",
    "- To showcase how the model generates context-aware and citation-rich responses when provided with external knowledge.\n",
    "- The improved RAG prompt enhances the model's ability to provide accurate, relevant, and coherent answers.\n",
    "\n",
    "### Expected Output\n",
    "- The response will cite specific paragraphs from the provided context using `[p{number}]` notation.\n",
    "- It will synthesize information across multiple paragraphs, maintaining logical coherence.\n",
    "\n",
    "### Comparison with Previous Outputs\n",
    "- This output demonstrates the model's improved reasoning and accuracy when augmented with external data via RAG, compared to its default knowledge-based response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393934e-ef6a-438a-b38e-3b819b98dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.12 Generating a Response with the Improved RAG Prompt\n",
    "# Query the model with the improved RAG prompt, which includes retrieved context and structured instructions.\n",
    "response = stream_markdown(\n",
    "    'llama3.2:3b',  # Model name\n",
    "    improved_rag_prompt  # Enhanced RAG prompt combining the query and retrieved context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a6667-dcd2-4fdf-9ef7-14811c759ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
