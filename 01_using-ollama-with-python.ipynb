{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ec6ff8-08f5-4ecd-9359-30f25c220898",
   "metadata": {},
   "source": [
    "# 01 Using Ollama with Python\n",
    "\n",
    "This Jupyter notebook provides a comprehensive demonstration of how to use the [Ollama Python library](https://github.com/ollama/ollama-python).\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- How to set up and initialize the Ollama library.\n",
    "- Some of the core functionalities of Ollama Libarry with examples.\n",
    "\n",
    "Each code cell is documented in detail to guide you through the steps, making it easy to adapt the code for your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efeec4-5fe7-4184-bfe9-313f0a7c136a",
   "metadata": {},
   "source": [
    "## 01.01 Installing the Ollama Library\n",
    "\n",
    "This cell installs the **Ollama** library using the `%pip install` command, a Jupyter magic command specifically for managing Python packages. \n",
    "\n",
    "- **Purpose**: The Ollama library is used for accessing its functionalities in Python, such as interacting with models and performing tasks outlined in the [official Ollama Python documentation](https://github.com/ollama/ollama-python).\n",
    "- **Why `%pip`**: Using `%pip install` instead of `pip install` ensures that the library is installed in the Python environment currently used by the Jupyter notebook. This is particularly useful when working in isolated environments like virtual environments or Jupyter kernels.\n",
    "\n",
    "If the Ollama library is already installed, this command will verify its presence and skip reinstallation. Otherwise, it fetches and installs the latest version from PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d57c5a-90ce-4772-a3ca-1b5c3d7a463d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 01.01 Installing the Ollama Library\n",
    "\n",
    "# Using the Jupyter magic command %pip to install the Ollama library.\n",
    "# This command ensures that the package is installed in the current Jupyter environment.\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f59e74-1964-4cf0-a104-c2c566198bf4",
   "metadata": {},
   "source": [
    "## 01.02 Importing the Ollama Library\n",
    "\n",
    "This cell imports the **Ollama** library, which is required to use its functionalities in Python.\n",
    "\n",
    "- **Purpose**: The Ollama library provides an API for interacting with models. By importing the library, you gain access to methods for listing available models, loading specific models, and performing tasks such as generating or processing data.\n",
    "- **Why This Step Is Needed**: Before using any of the library's features, it must first be imported into the Python environment.\n",
    "\n",
    "Make sure the library is installed (as shown in the earlier installation step) before running this cell to avoid import errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db488d36-9436-4dc1-bbd7-b5cf68400faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.02 Importing the Ollama Library\n",
    "\n",
    "# Importing the Ollama library to access its features and functionalities.\n",
    "# This library enables interaction with models, including listing, loading, and executing them.\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2700e3-12db-4c25-be9e-08eb84dbb427",
   "metadata": {},
   "source": [
    "## 01.03 Listing Available Models in the Ollama Library\n",
    "\n",
    "This cell demonstrates how to retrieve and display the list of models available in the Ollama library.\n",
    "\n",
    "1. **`ollama.list()`**:\n",
    "   - This function fetches all the models currently accessible through the Ollama library.\n",
    "   - The returned object contains detailed information about each model.\n",
    "\n",
    "2. **Iterating through the Models**:\n",
    "   - The `models` attribute of the `available_models` object contains a collection of models.\n",
    "   - Using a `for` loop, each model is accessed and its name (stored in the `model` attribute) is printed.\n",
    "\n",
    "**Output**:\n",
    "This cell outputs the names of the available models in a human-readable format. For example:\n",
    "\n",
    "```\n",
    "    model_1\n",
    "    model_2\n",
    "```\n",
    "**Use Case**:\n",
    "This step is useful to confirm which models are installed and available for use, helping you select the appropriate one for your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ef0f4-793c-4a60-801e-3ddbeb944959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.03 Listing Available Models in the Ollama Library\n",
    "\n",
    "# Fetching the list of available models from the Ollama library.\n",
    "available_models = ollama.list()\n",
    "\n",
    "# Iterating through the list of models and printing their names.\n",
    "# Each model is accessed through the 'models' attribute of the 'available_models' object.\n",
    "for model in available_models.models:\n",
    "    print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8861887-b59e-404a-b349-69ec70f67037",
   "metadata": {},
   "source": [
    "## 01.04 Generating a Model Response\n",
    "\n",
    "This cell demonstrates how to generate a response using the Ollama library and verify the raw output for debugging purposes.\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Ensure the `selected_model` variable contains a valid model name retrieved from the list of available models. In this example, the model is `'llama3.2:3b'`.\n",
    "\n",
    "2. **Input Prompt**:\n",
    "   - The `user_prompt` variable defines the query or task for the model. Here, the prompt asks: `\"Why is the sky blue?\"`.\n",
    "\n",
    "3. **Generating the Response**:\n",
    "   - The `ollama.generate()` function processes the query with the selected model.\n",
    "   - **Arguments**:\n",
    "     - `model`: The name of the model to be used. Ensure it matches a model retrieved from the earlier `ollama.list()` output.\n",
    "     - `prompt`: The user-provided input or query.\n",
    "\n",
    "4. **Raw Output for Debugging**:\n",
    "   - The `print(model_response)` statement displays the unformatted response from the model.\n",
    "   - **Note**: The printed output in this step is primarily for debugging purposes. It may not be user-friendly or visually neat.\n",
    "   - **Next Step**: A more detailed and structured display of the model's response will be implemented in the subsequent sections.\n",
    "\n",
    "**Important**:\n",
    "Verify that the `selected_model` is one of the models returned by `ollama.list()` to avoid errors or unexpected results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027a733-39ea-478a-81d8-a8d3b51b0f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 01.04 Generating a Model Response\n",
    "\n",
    "# Generating a response from the specified model using Ollama.\n",
    "# Make sure the model name ('llama3.2:3b' in this case) matches one of the models listed earlier.\n",
    "# The prompt contains the input question to be processed by the model.\n",
    "selected_model = 'llama3.2:3b'  # Replace with a model name from the list of available models.\n",
    "user_prompt = 'Why is the sky blue?'  # The input prompt or query for the model.\n",
    "\n",
    "# Requesting the model to generate a response based on the prompt.\n",
    "model_response = ollama.generate(model=selected_model, prompt=user_prompt)\n",
    "\n",
    "# Printing the raw response for debugging purposes.\n",
    "# Note: The output may not be formatted neatly here; detailed formatting is covered in the next section.\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af31e51-2713-47ea-8951-43e3e53aff09",
   "metadata": {},
   "source": [
    "## 01.05 Displaying the Response in Markdown Format\n",
    "\n",
    "This cell displays the model's response in a structured and visually appealing format using Markdown in a Jupyter notebook.\n",
    "\n",
    "1. **`IPython.display`**:\n",
    "   - The `display` function is used to render rich content like Markdown, HTML, or images directly in a Jupyter notebook.\n",
    "\n",
    "2. **`Markdown()`**:\n",
    "   - Converts plain text into Markdown for rendering.\n",
    "   - Assumes the `model_response.response` contains text formatted as Markdown or plain text compatible with Markdown rendering.\n",
    "\n",
    "3. **Purpose**:\n",
    "   - Instead of viewing raw output from the `print()` statement, this method provides a neatly formatted display.\n",
    "   - This is particularly useful for responses containing structured content, such as headings, lists, or links.\n",
    "  \n",
    "**Important**:\n",
    "Ensure the `model_response.response` attribute exists and contains valid text. If the response structure differs, adjust the code to access the appropriate attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53aa4c-25a8-4772-96fd-01e886196648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 01.05 Displaying the Response in Markdown Format\n",
    "\n",
    "# Importing the necessary function to display rich Markdown content in a Jupyter notebook.\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Displaying the response text in a structured Markdown format.\n",
    "# This assumes the model's response contains a 'response' attribute with Markdown-compatible content.\n",
    "display(Markdown(model_response.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07083b95-4c06-42e1-bcf2-3d4559d66cb9",
   "metadata": {},
   "source": [
    "## 01.06 Streaming Model Responses Using Ollama's `chat` Function\n",
    "\n",
    "This cell demonstrates how to interact with a model using Ollama's `chat` function, stream the response in real time, and manage the user input as a separate variable for clarity and reusability.\n",
    "\n",
    "1. **Prompt Variable**:\n",
    "   - The `user_prompt` variable stores the input question (`'Why is the sky blue?'` in this example).\n",
    "   - This makes the code more modular and easier to modify or reuse with different prompts.\n",
    "\n",
    "2. **`chat` Function**:\n",
    "   - Facilitates a conversational interaction with the specified model.\n",
    "   - **Arguments**:\n",
    "     - `model`: Specifies the model to use (e.g., `'llama3.2:3b'`). Ensure it matches a model retrieved from the `ollama.list()` output.\n",
    "     - `messages`: A list containing the conversation history. The prompt is passed as a dictionary under the `'content'` key.\n",
    "\n",
    "3. **Streaming the Response**:\n",
    "   - The `stream` parameter, when set to `True`, enables real-time response generation in chunks.\n",
    "   - Iterating over `stream` yields each chunk, which is immediately displayed.\n",
    "\n",
    "4. **Real-Time Output**:\n",
    "   - The `print()` function outputs the response chunk by chunk as it is received.\n",
    "   - Using `end=''` prevents newline characters between chunks, and `flush=True` ensures the output is displayed without buffering delays.\n",
    "\n",
    "**Advantages**:\n",
    "- Real-time streaming provides immediate feedback, useful for long responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14592a-c6a4-415f-94be-10e0fa8bea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.06 Streaming Model Responses Using Ollama's chat Function\n",
    "\n",
    "# Importing the `chat` function from the Ollama library to interact with a model in a chat-like format.\n",
    "from ollama import chat\n",
    "\n",
    "# Define the prompt as a variable for better reusability and readability.\n",
    "user_prompt = 'Why is the sky blue?'  # User's input question for the model.\n",
    "\n",
    "# Initiating a streaming chat session with the specified model and user message.\n",
    "# 'model' specifies the model to be used, and 'messages' provides the conversation history.\n",
    "stream = chat(\n",
    "    model='llama3.2:3b',  # Replace with a valid model name from the list of available models.\n",
    "    messages=[{'role': 'user', 'content': user_prompt}],  # Include the prompt in the messages list.\n",
    "    stream=True,  # Enables streaming mode to receive the response in chunks.\n",
    ")\n",
    "\n",
    "# Iterating through the streamed chunks of the response.\n",
    "# This allows for real-time display of the response as it is generated.\n",
    "for chunk in stream:\n",
    "    # Printing each chunk of the response content without adding a newline.\n",
    "    # Using 'flush=True' ensures immediate output display without buffering.\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1abcc-a224-4844-b8a1-2eb3b2547941",
   "metadata": {},
   "source": [
    "## 01.07 Retrieving Model Information with `ollama.show`\n",
    "\n",
    "This cell demonstrates how to serialize and display detailed information about a model retrieved from the Ollama library using the `modelinfo` attribute.\n",
    "\n",
    "1. **Retrieving Model Information**:\n",
    "   - The `ollama.show()` function fetches metadata about the specified model.\n",
    "   - The `.modelinfo` attribute accesses the detailed information specific to the model.\n",
    "\n",
    "2. **JSON Serialization**:\n",
    "   - The `json.dumps()` function converts the `modelinfo` Python object into a JSON-formatted string.\n",
    "   - **Arguments**:\n",
    "     - `model_info`: The Python object containing the model's metadata.\n",
    "     - `indent=4`: Adds indentation to the JSON string for improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f7d39-a570-4d89-8649-28fd64497883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.07 Retrieving Model Information with ollama.show\n",
    "\n",
    "# Importing the json module to handle JSON serialization.\n",
    "import json\n",
    "\n",
    "# Retrieving detailed information about the specified model using Ollama's 'show' function.\n",
    "# Accessing the 'modelinfo' attribute for the specific model details.\n",
    "model_info = ollama.show('llama3.2:3b').modelinfo  # Replace with a valid model name.\n",
    "\n",
    "# Serializing the Python object to JSON format.\n",
    "# Converts the Python object into a JSON-formatted string with a readable indentation of 4 spaces.\n",
    "json_model_info = json.dumps(model_info, indent=4)\n",
    "\n",
    "# Printing the JSON-formatted string for display.\n",
    "print(json_model_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
